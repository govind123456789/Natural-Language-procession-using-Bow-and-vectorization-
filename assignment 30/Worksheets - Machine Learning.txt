				MACHINE LEARNING WORKSHEET – 2 
In Q1 to Q5, only one option is correct, Choose the correct option: 
1. In which of the following you can say that the model is overfitting?
 A) High R-squared value for train-set and High R-squared value for test-set. B) Low R-squared value for train-set and High R-squared value for test-set. C) High R-squared value for train-set and Low R-squared value for test-set. D) None of the above
Answer-   C) High R-squared value for train-set and Low R-squared value for test-set
 2. Which among the following is a disadvantage of decision trees?     
 A) Decision trees are prone to outliers. B) Decision trees are highly prone to overfitting. C) Decision trees are not easy to interpret D) None of the above.
Answer-  . B) Decision trees are highly prone to overfitting
 3. Which of the following is an ensemble technique?   
  A) SVM      B) Logistic Regression C) Random Forest     D) Decision tree 
Answer-   C) Random Forest     
4. Suppose you are building a classification model for detection of a fatal disease where detection of the disease is most important. In this case which of the following metrics you would focus on? A) Accuracy      B) Sensitivity	 C) Precision      D) None of the above
Answer-   A) Accuracy  
 5. The value of AUC (Area under Curve) value for ROC curve of model A is 0.70 and of model B is 0.85. Which of these two models is doing better job in classification? 
A) Model A      B) Model B  C) both are performing equal    D) Data Insufficient
Answer- B) Model B  
 In Q6 to Q9, more than one options are correct, Choose all the correct options: 
6. Which of the following are the regularization technique in Linear Regression?? 
 A) Ridge      B) R-squared    C) MSE       D) Lasso
Answer-  A) Ridge      D) Lasso
 7. Which of the following is not an example of boosting technique?    
  A) Adaboost      B) Decision Tree C) Random Forest     D) Xgboost. 
Answer-    A) Adaboost    D) Xgboost. 
8. Which of the techniques are used for regularization of Decision Trees? 
 A) Pruning      B) L2 regularization C) Restricting the max depth of the tree   D) All of the above 
Answer- D) All of the above 
9. Which of the following statements is true regarding the Adaboost technique? 
 A) We initialize the probabilities of the distribution as 1/n, where n is the number of data-points B) A tree in the ensemble focuses more on the data points on which the previous tree was not performing well C) It is example of bagging technique D) None of the above
Answer- A) We initialize the probabilities of the distribution as 1/n, where n is the number of data-points   C) It is example of bagging technique 
 Q10 to Q15 are subjective answer type questions, Answer them briefly. 
10. Explain how does the adjusted R-squared penalize the presence of unnecessary predictors in the model?
Answer-  The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance
 11. Differentiate between Ridge and Lasso Regression.
Answer-  Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function. Here the highlighted part represents L2 regularization element.
Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
Answer
 12. What is VIF? What is the suitable value of a VIF for a feature to be included in a regression modelling?
Answer- A variance inflation factor (VIF) provides a measure of multicollinearity among the independent variables in a multiple regression model.Typically in practice there is a small amount of collinearity among the predictors. As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity" .
 13. Why do we need to scale the data before feeding it to the train the model?
Answer- Given the use of small weights in the model and the use of error between predictions and expected values, the scale of inputs and outputs used to train the model are an important factor. Unscaled input variables can result in a slow or unstable learning process, whereas unscaled target variables on regression problems can result in exploding gradients causing the learning process to fail
 14. What are the different metrics which are used to check the goodness of fit in linear regression? 
Answer- A goodness-of-fit test, in general, refers to measuring how well do the observed data correspond to the fitted model. We will use this concept throughout the course as a way of checking the model fit. Like in a linear regression, in essence, the goodness-of-fit test compares the observed values to the expected values.
15. From the following confusion matrix calculate sensitivity, specificity, precision, recall and accuracy.  
 Actual/Predicted 		True 		False
 True 				1000 		50 
False 				250		 1200 
Answer-confusion matrix accuracy=  1000+1200/1000+1200+250+50=0.88
	confusion matrix Recall = 1000/1000+50 = 0.95
	confusion matrix precision= 1000/1000+250 =0.8
	confusion matrix sensitivity = 1000/1000+50=0.95
	confusion matrix specificity=1200/1200+250=0.82
			 